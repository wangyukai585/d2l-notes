前言：

在本章中，我们将第一次介绍真正的_深度_网络。 最简单的深度网络称为_多层感知机_。

多层感知机由多层神经元组成， 每一层与它的上一层相连，从中接收输入； 同时每一层也与它的下一层相连，影响当前层的神经元。

_过拟合_、_欠拟合_和模型选择

本章将介绍_权重衰减_和_暂退法_等正则化技术

数值稳定性和参数初始化相关的问题

房价预测

---

## 4.1.1 隐藏层

仿射变换中的_线性_是一个很强的假设。

### 线下模型可能会出错

线性意味着_单调_假设： 任何特征的增大都会导致模型输出的增大（如果对应的权重为正）， 或者导致模型输出的减小（如果对应的权重为负）。

### 在网络中加入隐藏层

将许多全连接层堆叠在一起

我们可以把前L-1层看作表示，把最后一层看作线性预测器。 这种架构通常称为_多层感知机_（multilayer perceptron），通常缩写为_MLP_。

![[Pasted image 20251029185354.png]]

多层感知机中的层数为2

### 从线性到非线性

 对于具有h个隐藏单元的单隐藏层多层感知机， 用$\mathbf{H} \in \mathbb{R}^{n \times h}$表示隐藏层的输出， 称为_隐藏表示_（hidden representations）。
 H 隐藏层变量/隐藏变量

在仿射变换之后对每个隐藏单元应用非线性的_激活函数_（activation function）sigma

$$
\begin{split}\begin{aligned}
    \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\
\end{aligned}\end{split}
$$

### 通用近似定理

多层感知机是通用近似器。 即使是网络只有一个隐藏层，给定足够的神经元和正确的权重， 我们可以对任意函数建模，尽管实际中学习该函数是很困难的。

## 4.1.2 激活函数

_激活函数_（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。

### ReLU函数

_修正线性单元_（Rectified linear unit，_ReLU_）

$\operatorname{ReLU}(x) = \max(x, 0).$

如果微妙的边界条件很重要，我们很可能是在研究数学而非工程

_参数化ReLU_（Parameterized ReLU，_pReLU_）
$\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x).$

### sigmoid函数

_sigmoid函数_将输入变换为区间(0, 1)上的输出。 因此，sigmoid通常称为_挤压函数_（squashing function）： 它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：
$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.$

$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.$

### tanh函数

tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上

$\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.$
$\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).$

## 4.1.3 小结

- 多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。
    
- 常用的激活函数包括ReLU函数、sigmoid函数和tanh函数。