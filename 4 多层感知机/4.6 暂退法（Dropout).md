æˆ‘ä»¬ä»‹ç»äº†é€šè¿‡æƒ©ç½šæƒé‡çš„ğ¿2èŒƒæ•°æ¥æ­£åˆ™åŒ–ç»Ÿè®¡æ¨¡å‹çš„ç»å…¸æ–¹æ³•ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹è®ºè¯æ¥è¯æ˜è¿™ä¸€æŠ€æœ¯çš„åˆç†æ€§ï¼š æˆ‘ä»¬å·²ç»å‡è®¾äº†ä¸€ä¸ªå…ˆéªŒï¼Œå³æƒé‡çš„å€¼å–è‡ªå‡å€¼ä¸º0çš„é«˜æ–¯åˆ†å¸ƒã€‚ æ›´ç›´è§‚çš„æ˜¯ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹æ·±åº¦æŒ–æ˜ç‰¹å¾ï¼Œå³å°†å…¶æƒé‡åˆ†æ•£åˆ°è®¸å¤šç‰¹å¾ä¸­ï¼Œ è€Œä¸æ˜¯è¿‡äºä¾èµ–å°‘æ•°æ½œåœ¨çš„è™šå‡å…³è”ã€‚

## é‡æ–°å®¡è§†è¿‡æ‹Ÿåˆ

çº¿æ€§æ¨¡å‹æ²¡æœ‰è€ƒè™‘åˆ°ç‰¹å¾ä¹‹é—´çš„äº¤äº’ä½œç”¨ã€‚ å¯¹äºæ¯ä¸ªç‰¹å¾ï¼Œçº¿æ€§æ¨¡å‹å¿…é¡»æŒ‡å®šæ­£çš„æˆ–è´Ÿçš„æƒé‡ï¼Œè€Œå¿½ç•¥å…¶ä»–ç‰¹å¾ã€‚

æ³›åŒ–æ€§å’Œçµæ´»æ€§ä¹‹é—´çš„è¿™ç§åŸºæœ¬æƒè¡¡è¢«æè¿°ä¸º_åå·®-æ–¹å·®æƒè¡¡_ï¼ˆbias-variance tradeoffï¼‰ã€‚ çº¿æ€§æ¨¡å‹æœ‰å¾ˆé«˜çš„åå·®ï¼šå®ƒä»¬åªèƒ½è¡¨ç¤ºä¸€å°ç±»å‡½æ•°ã€‚ ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„æ–¹å·®å¾ˆä½ï¼šå®ƒä»¬åœ¨ä¸åŒçš„éšæœºæ•°æ®æ ·æœ¬ä¸Šå¯ä»¥å¾—å‡ºç›¸ä¼¼çš„ç»“æœã€‚

æ·±åº¦ç¥ç»ç½‘ç»œä½äºåå·®-æ–¹å·®è°±çš„å¦ä¸€ç«¯ã€‚ ä¸çº¿æ€§æ¨¡å‹ä¸åŒï¼Œç¥ç»ç½‘ç»œå¹¶ä¸å±€é™äºå•ç‹¬æŸ¥çœ‹æ¯ä¸ªç‰¹å¾ï¼Œè€Œæ˜¯å­¦ä¹ ç‰¹å¾ä¹‹é—´çš„äº¤äº’ã€‚

## æ‰°åŠ¨çš„ç¨³å¥æ€§

ç»å…¸æ³›åŒ–ç†è®ºè®¤ä¸ºï¼Œä¸ºäº†ç¼©å°è®­ç»ƒå’Œæµ‹è¯•æ€§èƒ½ä¹‹é—´çš„å·®è·ï¼Œåº”è¯¥ä»¥ç®€å•çš„æ¨¡å‹ä¸ºç›®æ ‡ã€‚æ­¤å¤–ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨ :numref:`sec_weight_decay`Â ä¸­è®¨è®ºæƒé‡è¡°å‡ï¼ˆğ¿2æ­£åˆ™åŒ–ï¼‰æ—¶çœ‹åˆ°çš„é‚£æ ·ï¼Œ å‚æ•°çš„èŒƒæ•°ä¹Ÿä»£è¡¨äº†ä¸€ç§æœ‰ç”¨çš„ç®€å•æ€§åº¦é‡ã€‚
ç®€å•æ€§çš„å¦ä¸€ä¸ªè§’åº¦æ˜¯å¹³æ»‘æ€§ï¼Œå³å‡½æ•°ä¸åº”è¯¥å¯¹å…¶è¾“å…¥çš„å¾®å°å˜åŒ–æ•æ„Ÿã€‚

Â åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»–ä»¬å»ºè®®åœ¨è®¡ç®—åç»­å±‚ä¹‹å‰å‘ç½‘ç»œçš„æ¯ä¸€å±‚æ³¨å…¥å™ªå£°ã€‚ å› ä¸ºå½“è®­ç»ƒä¸€ä¸ªæœ‰å¤šå±‚çš„æ·±å±‚ç½‘ç»œæ—¶ï¼Œæ³¨å…¥å™ªå£°åªä¼šåœ¨è¾“å…¥-è¾“å‡ºæ˜ å°„ä¸Šå¢å¼ºå¹³æ»‘æ€§ã€‚è¿™ä¸ªæƒ³æ³•è¢«ç§°ä¸º_æš‚é€€æ³•_ï¼ˆdropoutï¼‰ã€‚


_æ— åå‘_ï¼ˆunbiasedï¼‰çš„æ–¹å¼
è¿™æ ·åœ¨å›ºå®šä½å…¶ä»–å±‚æ—¶ï¼Œæ¯ä¸€å±‚çš„æœŸæœ›å€¼ç­‰äºæ²¡æœ‰å™ªéŸ³æ—¶çš„å€¼ã€‚

åœ¨æ ‡å‡†æš‚é€€æ³•æ­£åˆ™åŒ–ä¸­ï¼Œé€šè¿‡æŒ‰ä¿ç•™ï¼ˆæœªä¸¢å¼ƒï¼‰çš„èŠ‚ç‚¹çš„åˆ†æ•°è¿›è¡Œè§„èŒƒåŒ–æ¥æ¶ˆé™¤æ¯ä¸€å±‚çš„åå·®ã€‚
æ¢è¨€ä¹‹ï¼Œæ¯ä¸ªä¸­é—´æ´»æ€§å€¼$h$ä»¥*æš‚é€€æ¦‚ç‡*$p$ç”±éšæœºå˜é‡$h'$æ›¿æ¢ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

$$
\begin{aligned}
h' =
\begin{cases}
    0 & \text{ æ¦‚ç‡ä¸º } p \\
    \frac{h}{1-p} & \text{ å…¶ä»–æƒ…å†µ}
\end{cases}
\end{aligned}
$$

æ ¹æ®æ­¤æ¨¡å‹çš„è®¾è®¡ï¼Œå…¶æœŸæœ›å€¼ä¿æŒä¸å˜ï¼Œå³$E[h'] = h$ã€‚

## å®éªŒä¸­çš„æš‚é€€æ³•

é€šå¸¸ï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•æ—¶ä¸ç”¨æš‚é€€æ³•ã€‚ ç»™å®šä¸€ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹å’Œä¸€ä¸ªæ–°çš„æ ·æœ¬ï¼Œæˆ‘ä»¬ä¸ä¼šä¸¢å¼ƒä»»ä½•èŠ‚ç‚¹ï¼Œå› æ­¤ä¸éœ€è¦æ ‡å‡†åŒ–ã€‚

è¿™æ ·ï¼Œè¾“å‡ºå±‚çš„è®¡ç®—ä¸èƒ½è¿‡åº¦ä¾èµ–äºâ„1,â€¦,â„5çš„ä»»ä½•ä¸€ä¸ªå…ƒç´ ã€‚

## ä»é›¶å¼€å§‹å®ç°

```python
def dropout_layer(X, dropout):
    assert 0 <= dropout <= 1
    # åœ¨æœ¬æƒ…å†µä¸­ï¼Œæ‰€æœ‰å…ƒç´ éƒ½è¢«ä¸¢å¼ƒ
    if dropout == 1:
        return torch.zeros_like(X)
    # åœ¨æœ¬æƒ…å†µä¸­ï¼Œæ‰€æœ‰å…ƒç´ éƒ½è¢«ä¿ç•™
    if dropout == 0:
        return X
    mask = (torch.rand(X.shape) > dropout).float()
    return mask * X / (1.0 - dropout)
```

## å®šä¹‰æ¨¡å‹å‚æ•°

```python
num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256
```

## å®šä¹‰æ¨¡å‹

æˆ‘ä»¬å¯ä»¥å°†æš‚é€€æ³•åº”ç”¨äºæ¯ä¸ªéšè—å±‚çš„è¾“å‡ºï¼ˆåœ¨æ¿€æ´»å‡½æ•°ä¹‹åï¼‰ï¼Œ å¹¶ä¸”å¯ä»¥ä¸ºæ¯ä¸€å±‚åˆ†åˆ«è®¾ç½®æš‚é€€æ¦‚ç‡ï¼š å¸¸è§çš„æŠ€å·§æ˜¯åœ¨é è¿‘è¾“å…¥å±‚çš„åœ°æ–¹è®¾ç½®è¾ƒä½çš„æš‚é€€æ¦‚ç‡ã€‚ 

ä¸‹é¢çš„æ¨¡å‹å°†ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªéšè—å±‚çš„æš‚é€€æ¦‚ç‡åˆ†åˆ«è®¾ç½®ä¸º0.2å’Œ0.5ï¼Œ å¹¶ä¸”æš‚é€€æ³•åªåœ¨è®­ç»ƒæœŸé—´æœ‰æ•ˆã€‚

```python
dropout1, dropout2 = 0.2, 0.5

class Net(nn.Module):
    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,
                 is_training = True):
        super(Net, self).__init__()
        self.num_inputs = num_inputs
        self.training = is_training
        self.lin1 = nn.Linear(num_inputs, num_hiddens1)
        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)
        self.lin3 = nn.Linear(num_hiddens2, num_outputs)
        self.relu = nn.ReLU()

    def forward(self, X):
        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))
        # åªæœ‰åœ¨è®­ç»ƒæ¨¡å‹æ—¶æ‰ä½¿ç”¨dropout
        if self.training == True:
            # åœ¨ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚ä¹‹åæ·»åŠ ä¸€ä¸ªdropoutå±‚
            H1 = dropout_layer(H1, dropout1)
        H2 = self.relu(self.lin2(H1))
        if self.training == True:
            # åœ¨ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚ä¹‹åæ·»åŠ ä¸€ä¸ªdropoutå±‚
            H2 = dropout_layer(H2, dropout2)
        out = self.lin3(H2)
        return out


net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)
```

## è®­ç»ƒå’Œæµ‹è¯•

```python
num_epochs, lr, batch_size = 10, 0.5, 256
loss = nn.CrossEntropyLoss(reduction='none')
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
trainer = torch.optim.SGD(net.parameters(), lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

## ç®€æ´å®ç°

æ·±åº¦å­¦ä¹ æ¡†æ¶çš„é«˜çº§API

```python
net = nn.Sequential(nn.Flatten(),
        nn.Linear(784, 256),
        nn.ReLU(),
        # åœ¨ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚ä¹‹åæ·»åŠ ä¸€ä¸ªdropoutå±‚
        nn.Dropout(dropout1),
        nn.Linear(256, 256),
        nn.ReLU(),
        # åœ¨ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚ä¹‹åæ·»åŠ ä¸€ä¸ªdropoutå±‚
        nn.Dropout(dropout2),
        nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);
```

```python
trainer = torch.optim.SGD(net.parameters(), lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

## å°ç»“

- æš‚é€€æ³•åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œè®¡ç®—æ¯ä¸€å†…éƒ¨å±‚çš„åŒæ—¶ä¸¢å¼ƒä¸€äº›ç¥ç»å…ƒã€‚
- æš‚é€€æ³•å¯ä»¥é¿å…è¿‡æ‹Ÿåˆï¼Œå®ƒé€šå¸¸ä¸æ§åˆ¶æƒé‡å‘é‡çš„ç»´æ•°å’Œå¤§å°ç»“åˆä½¿ç”¨çš„ã€‚
- æš‚é€€æ³•å°†æ´»æ€§å€¼â„æ›¿æ¢ä¸ºå…·æœ‰æœŸæœ›å€¼â„çš„éšæœºå˜é‡ã€‚
- æš‚é€€æ³•ä»…åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨ã€‚

