
通过将基于模型的决策引入环境，我们可能会破坏模型。

## 分布偏移的类型

假设训练数据是从某个分布𝑝𝑆(𝐱,𝑦)中采样的， 但是测试数据将包含从不同分布𝑝𝑇(𝐱,𝑦)中抽取的未标记样本。 一个清醒的现实是：如果没有任何关于𝑝𝑆和𝑝𝑇之间相互关系的假设， 学习到一个分类器是不可能的。

### 协变量偏移

虽然输入的分布可能随时间而改变， 但标签函数（即条件分布𝑃(𝑦∣𝐱)）没有改变。 统计学家称之为_协变量偏移_（covariate shift）， 因为这个问题是由于协变量（特征）分布的变化而产生的。

### 标签偏移

_标签偏移_（label shift）描述了与协变量偏移相反的问题。 这里我们假设标签边缘概率𝑃(𝑦)可以改变， 但是类别条件分布𝑃(𝐱∣𝑦)在不同的领域之间保持不变。

### 概念偏移

当标签的定义发生变化时，就会出现这种问题

## 分布偏移示例

医学诊断 协变量偏移

自动驾驶汽车

非平稳分布

## 分布偏移纠正

### 经验风险与实际风险

 _经验风险_（empirical risk）是为了近似 _真实风险_（true risk）， 整个训练数据上的平均损失，即从其真实分布𝑝(𝐱,𝑦)中 抽取的所有数据的总体损失的期望值

$$\mathop{\mathrm{minimize}}_f \frac{1}{n} \sum_{i=1}^n l(f(\mathbf{x}_i), y_i),$$
其中$l$是损失函数，用来度量：
给定标签$y_i$，预测$f(\mathbf{x}_i)$的“糟糕程度”。

### 协变量偏移纠正

又是一堆数学推导
$$
\iint \ell(f(x),y)\,p(y\mid x)\,p(x)\,dx\,dy =\iint \ell(f(x),y)\,q(y\mid x)\,q(x)\,\underbrace{\frac{p(x)}{q(x)}}_{\beta(x)}\,dx\,dy.
$$


### 概念偏移纠正

我们使用新数据更新现有的网络权重，而不是从头开始训练。

## 学习问题的分类法

### 批量学习

- 一次性拿到所有训练数据 (x_1, y_1), \dots, (x_n, y_n)。
- 先把模型 f(x) 训练好，再部署使用。
- 部署后**基本不再更新**。

## 在线学习

- 数据一条一条来，比如 (x_i, y_i)。
- 每次先根据 x_i 做预测 f(x_i)，再看到真实结果 y_i。
- 根据这次误差更新模型参数，准备预测下一次。

$$
\mathrm{model} ~ f_t \longrightarrow
\mathrm{data} ~ \mathbf{x}_t \longrightarrow
\mathrm{estimate} ~ f_t(\mathbf{x}_t) \longrightarrow
\mathrm{observation} ~ y_t \longrightarrow
\mathrm{loss} ~ l(y_t, f_t(\mathbf{x}_t)) \longrightarrow
\mathrm{model} ~ f_{t+1}
$$

### 老虎机

**老虎机（bandit）问题**是一种特殊的机器学习问题：
- 你有**有限个动作**（比如几根老虎机拉杆，或几种策略）可以选择；
- 每次选择一个动作，会得到一个随机奖励（赢或输）；
- 目标是**通过尝试不同动作，找到平均收益最高的那个**。

### 控制

在很多情况下，环境会记住我们所做的事。 不一定是以一种对抗的方式，但它会记住，而且它的反应将取决于之前发生的事情。

这种“有记忆”的系统属于**控制问题（control）**。
- 传统做法是用 **PID 控制器**：根据过去的误差调整当前动作。
- 在机器学习中，也有类似思想，比如用控制算法自动调整**学习率、超参数**，
从而提高模型的稳定性、多样性或生成质量。

### 强化学习

_强化学习_（reinforcement learning）强调如何基于环境而行动，以取得最大化的预期利益。 国际象棋、围棋、西洋双陆棋或星际争霸都是强化学习的应用实例。

### 考虑到环境

上述不同情况之间的一个关键区别是： 在静止环境中可能一直有效的相同策略， 在环境能够改变的情况下可能不会始终有效。

## 机器学习中的公平、责任和透明度

我们会发现精度很少成为合适的衡量标准。

考虑到这些社会价值

通常，在建模纠正过程中，模型的预测与训练数据耦合的各种机制都没有得到解释， 研究人员称之为“失控反馈循环”的现象。此外，我们首先要注意我们是否解决了正确的问题。 比如，预测算法现在在信息传播中起着巨大的中介作用， 个人看到的新闻应该由他们喜欢的Facebook页面决定吗？ 这些只是在机器学习职业生涯中可能遇到的令人感到“压力山大”的道德困境中的一小部分。

## 小结

- 在许多情况下，训练集和测试集并不来自同一个分布。这就是所谓的分布偏移。
    
- 真实风险是从真实分布中抽取的所有数据的总体损失的预期。然而，这个数据总体通常是无法获得的。经验风险是训练数据的平均损失，用于近似真实风险。在实践中，我们进行经验风险最小化。
    
- 在相应的假设条件下，可以在测试时检测并纠正协变量偏移和标签偏移。在测试时，不考虑这种偏移可能会成为问题。
    
- 在某些情况下，环境可能会记住自动操作并以令人惊讶的方式做出响应。在构建模型时，我们必须考虑到这种可能性，并继续监控实时系统，并对我们的模型和环境以意想不到的方式纠缠在一起的可能性持开放态度。


## CHATGPT答疑解惑
- x：输入特征（图片、症状等）
- y：标签（猫/狗、是否生病等）
- $p(\cdot)$：**目标分布**（你真正关心的环境，比如将来上线或测试时的数据分布）
- $q(\cdot)$：**源分布**（你现在手里的训练数据来自哪里，它的分布）
- $p(y\mid x)$：在目标环境里，给定 x 时 y 的概率（“标注规律”）
- $\ell(f(x),y)$：损失，模型 f 在样本 (x,y) 上“错得有多离谱”

协变量偏移的**关键假设**：标注规律不变，只是样本出现频率变了：
$$
p(y\mid x)=q(y\mid x),\qquad \text{但 } p(x)\neq q(x).
$$
