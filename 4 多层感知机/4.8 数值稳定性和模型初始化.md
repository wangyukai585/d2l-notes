
我们选择哪个函数以及如何初始化参数可以决定优化算法收敛的速度有多快。 糟糕选择可能会导致我们在训练时遇到梯度爆炸或梯度消失。

## 梯度消失和梯度爆炸

 要么是_梯度爆炸_（gradient exploding）问题： 参数更新过大，破坏了模型的稳定收敛； 要么是_梯度消失_（gradient vanishing）问题： 参数更新过小，在每次更新时几乎不会移动，导致模型无法学习

### 梯度消失

![[Pasted image 20251030233151.png]]

### 梯度爆炸

![[Pasted image 20251030233302.png]]

## 打破对称性

神经网络设计中的另一个问题是其参数化所固有的对称性。
第一个隐藏单元与第二个隐藏单元没有什么特别的区别。 换句话说，我们在每一层的隐藏单元之间具有排列对称性。

想象一下，如果我们将隐藏层的所有参数初始化为𝐖(1)=𝑐， 𝑐为常量，会发生什么？ 在这种情况下，在前向传播期间，两个隐藏单元采用相同的输入和参数， 产生相同的激活，该激活被送到输出单元。 在反向传播期间，根据参数𝐖(1)对输出单元进行微分， 得到一个梯度，其元素都取相同的值。 因此，在基于梯度的迭代（例如，小批量随机梯度下降）之后， 𝐖(1)的所有元素仍然采用相同的值。

请注意，虽然小批量随机梯度下降不会打破这种对称性，但暂退法正则化可以。

## 参数初始化

解决（或至少减轻）上述问题的一种方法是进行参数初始化， 优化期间的注意和适当的正则化也可以进一步提高稳定性。

### 默认初始化

正态分布初始化

### Xavier初始化

在一个没有激活函数的全连接层中，

每个输出神经元 o_i 是所有输入 x_j 经过加权求和得到的：

$$  
o_i = \sum_{j=1}^{n_{in}} w_{ij} x_j
$$

解释：
- n_{in}：输入的特征数（上一层的神经元个数）；
- x_j：第 j 个输入；
- w_{ij}：从输入节点 j 到输出节点 i 的权重；
- o_i：第 i 个输出节点的值。

保持方差不变的一种方法是设置$n_\mathrm{in} \sigma^2 = 1$。

除非$n_\mathrm{out} \sigma^2 = 1$，
否则梯度的方差可能会增大，其中$n_\mathrm{out}$是该层的输出的数量。
这使得我们进退两难：我们不可能同时满足这两个条件。

相反，我们只需满足：

$$
\begin{aligned}
\frac{1}{2} (n_\mathrm{in} + n_\mathrm{out}) \sigma^2 = 1 \text{ 或等价于 }
\sigma = \sqrt{\frac{2}{n_\mathrm{in} + n_\mathrm{out}}}.
\end{aligned}
$$

## 小结

- 梯度消失和梯度爆炸是深度网络中常见的问题。在参数初始化时需要非常小心，以确保梯度和参数可以得到很好的控制。
- 需要用启发式的初始化方法来确保初始梯度既不太大也不太小。
- ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。
- 随机初始化是保证在进行优化前打破对称性的关键。
- Xavier初始化表明，对于每一层，输出的方差不受输入数量的影响，任何梯度的方差不受输出数量的影响。


“启发式”意思是：
不是严格推导出来的最优公式，而是**基于经验和直觉得到的有效方法**。