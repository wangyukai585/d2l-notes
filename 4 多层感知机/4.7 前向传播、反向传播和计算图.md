本节将通过一些基本的数学和计算图， 深入探讨_反向传播_的细节。 首先，我们将重点放在带权重衰减（𝐿2正则化）的单隐藏层多层感知机上。

## 前向传播

_前向传播_（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。

目标函数J=L+s

## 前向传播计算图

![[Pasted image 20251030230648.png]]

## 反向传播

_反向传播_（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。

一堆数学公式

## 训练神经网络

因此，在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。 注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。 带来的影响之一是我们需要保留中间值，直到反向传播完成。 这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。 因此，使用更大的批量来训练更深层次的网络更容易导致_内存不足_（out of memory）错误。

## 小结

- 前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。
- 反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。
- 在训练深度学习模型时，前向传播和反向传播是相互依赖的。
- 训练比预测需要更多的内存。
