ç®€è¨€ä¹‹ï¼Œå¦‚æœè¯´å·ç§¯ç¥ç»ç½‘ç»œå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†ç©ºé—´ä¿¡æ¯ï¼Œ é‚£ä¹ˆæœ¬ç« çš„_å¾ªç¯ç¥ç»ç½‘ç»œ_ï¼ˆrecurrent neural networkï¼ŒRNNï¼‰åˆ™å¯ä»¥æ›´å¥½åœ°å¤„ç†åºåˆ—ä¿¡æ¯ã€‚ å¾ªç¯ç¥ç»ç½‘ç»œé€šè¿‡å¼•å…¥çŠ¶æ€å˜é‡å­˜å‚¨è¿‡å»çš„ä¿¡æ¯å’Œå½“å‰çš„è¾“å…¥ï¼Œä»è€Œå¯ä»¥ç¡®å®šå½“å‰çš„è¾“å‡ºã€‚

---


åœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œå‰è€…ï¼ˆå¯¹è¶…å‡ºå·²çŸ¥è§‚æµ‹èŒƒå›´è¿›è¡Œé¢„æµ‹ï¼‰ç§°ä¸º_å¤–æ¨æ³•_ï¼ˆextrapolationï¼‰ï¼Œ è€Œåè€…ï¼ˆåœ¨ç°æœ‰è§‚æµ‹å€¼ä¹‹é—´è¿›è¡Œä¼°è®¡ï¼‰ç§°ä¸º_å†…æ’æ³•_ï¼ˆinterpolationï¼‰ã€‚

## ç»Ÿè®¡å·¥å…·

### è‡ªå›å½’æ¨¡å‹

æœ¬ç« åé¢çš„å¤§éƒ¨åˆ†å†…å®¹å°†å›´ç»•ç€å¦‚ä½•æœ‰æ•ˆä¼°è®¡Â ğ‘ƒ(ğ‘¥ğ‘¡âˆ£ğ‘¥ğ‘¡âˆ’1,â€¦,ğ‘¥1)å±•å¼€ã€‚ ç®€å•åœ°è¯´ï¼Œå®ƒå½’ç»“ä¸ºä»¥ä¸‹ä¸¤ç§ç­–ç•¥ã€‚

ç¬¬ä¸€ç§ç­–ç•¥ï¼Œå‡è®¾åœ¨ç°å®æƒ…å†µä¸‹ç›¸å½“é•¿çš„åºåˆ—Â ğ‘¥ğ‘¡âˆ’1,â€¦,ğ‘¥1å¯èƒ½æ˜¯ä¸å¿…è¦çš„ï¼Œ å› æ­¤æˆ‘ä»¬åªéœ€è¦æ»¡è¶³æŸä¸ªé•¿åº¦ä¸ºğœçš„æ—¶é—´è·¨åº¦ï¼Œ å³ä½¿ç”¨è§‚æµ‹åºåˆ—ğ‘¥ğ‘¡âˆ’1,â€¦,ğ‘¥ğ‘¡âˆ’ğœã€‚ å½“ä¸‹è·å¾—çš„æœ€ç›´æ¥çš„å¥½å¤„å°±æ˜¯å‚æ•°çš„æ•°é‡æ€»æ˜¯ä¸å˜çš„ï¼Œ è‡³å°‘åœ¨ğ‘¡>ğœæ—¶å¦‚æ­¤ï¼Œè¿™å°±ä½¿æˆ‘ä»¬èƒ½å¤Ÿè®­ç»ƒä¸€ä¸ªä¸Šé¢æåŠçš„æ·±åº¦ç½‘ç»œã€‚ è¿™ç§æ¨¡å‹è¢«ç§°ä¸º_è‡ªå›å½’æ¨¡å‹_ï¼ˆautoregressive modelsï¼‰ï¼Œ å› ä¸ºå®ƒä»¬æ˜¯å¯¹è‡ªå·±æ‰§è¡Œå›å½’ã€‚

ç¬¬äºŒç§ç­–ç•¥ï¼Œå¦‚ :numref:`fig_sequence-model`æ‰€ç¤ºï¼Œ æ˜¯ä¿ç•™ä¸€äº›å¯¹è¿‡å»è§‚æµ‹çš„æ€»ç»“â„ğ‘¡ï¼Œ å¹¶ä¸”åŒæ—¶æ›´æ–°é¢„æµ‹ğ‘¥Ì‚Â ğ‘¡å’Œæ€»ç»“â„ğ‘¡ã€‚ è¿™å°±äº§ç”Ÿäº†åŸºäºğ‘¥Ì‚Â ğ‘¡=ğ‘ƒ(ğ‘¥ğ‘¡âˆ£â„ğ‘¡)ä¼°è®¡ğ‘¥ğ‘¡ï¼Œ ä»¥åŠå…¬å¼â„ğ‘¡=ğ‘”(â„ğ‘¡âˆ’1,ğ‘¥ğ‘¡âˆ’1)æ›´æ–°çš„æ¨¡å‹ã€‚ ç”±äºâ„ğ‘¡ä»æœªè¢«è§‚æµ‹åˆ°ï¼Œè¿™ç±»æ¨¡å‹ä¹Ÿè¢«ç§°ä¸ºÂ _éšå˜é‡è‡ªå›å½’æ¨¡å‹_ï¼ˆlatent autoregressive modelsï¼‰ã€‚
![[Pasted image 20251101233722.png]]


ç»Ÿè®¡å­¦å®¶ç§°ä¸å˜çš„åŠ¨åŠ›å­¦ä¸º_é™æ­¢çš„_ï¼ˆstationaryï¼‰ã€‚

$$P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}, \ldots, x_1).$$
æ³¨æ„ï¼Œå¦‚æœæˆ‘ä»¬å¤„ç†çš„æ˜¯ç¦»æ•£çš„å¯¹è±¡ï¼ˆå¦‚å•è¯ï¼‰ï¼Œ è€Œä¸æ˜¯è¿ç»­çš„æ•°å­—ï¼Œåˆ™ä¸Šè¿°çš„è€ƒè™‘ä»ç„¶æœ‰æ•ˆã€‚ å”¯ä¸€çš„å·®åˆ«æ˜¯ï¼Œå¯¹äºç¦»æ•£çš„å¯¹è±¡ï¼Œ æˆ‘ä»¬éœ€è¦ä½¿ç”¨åˆ†ç±»å™¨è€Œä¸æ˜¯å›å½’æ¨¡å‹æ¥ä¼°è®¡ğ‘ƒ(ğ‘¥ğ‘¡âˆ£ğ‘¥ğ‘¡âˆ’1,â€¦,ğ‘¥1)ã€‚

### é©¬å°”ç§‘å¤«æ¨¡å‹

å›æƒ³ä¸€ä¸‹ï¼Œåœ¨è‡ªå›å½’æ¨¡å‹çš„è¿‘ä¼¼æ³•ä¸­ï¼Œ æˆ‘ä»¬ä½¿ç”¨ğ‘¥ğ‘¡âˆ’1,â€¦,ğ‘¥ğ‘¡âˆ’ğœÂ è€Œä¸æ˜¯ğ‘¥ğ‘¡âˆ’1,â€¦,ğ‘¥1æ¥ä¼°è®¡ğ‘¥ğ‘¡ã€‚ åªè¦è¿™ç§æ˜¯è¿‘ä¼¼ç²¾ç¡®çš„ï¼Œæˆ‘ä»¬å°±è¯´åºåˆ—æ»¡è¶³_é©¬å°”å¯å¤«æ¡ä»¶_ï¼ˆMarkov conditionï¼‰ã€‚

ç‰¹åˆ«æ˜¯ï¼Œå¦‚æœğœ=1ï¼Œå¾—åˆ°ä¸€ä¸ªÂ _ä¸€é˜¶é©¬å°”å¯å¤«æ¨¡å‹_ï¼ˆfirst-order Markov modelï¼‰ï¼ŒÂ ğ‘ƒ(ğ‘¥)ç”±ä¸‹å¼ç»™å‡ºï¼š

$$P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}) \text{ å½“ } P(x_1 \mid x_0) = P(x_1).$$

å½“å‡è®¾$x_t$ä»…æ˜¯ç¦»æ•£å€¼æ—¶ï¼Œè¿™æ ·çš„æ¨¡å‹ç‰¹åˆ«æ£’ï¼Œ
å› ä¸ºåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨åŠ¨æ€è§„åˆ’å¯ä»¥æ²¿ç€é©¬å°”å¯å¤«é“¾ç²¾ç¡®åœ°è®¡ç®—ç»“æœã€‚
ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥é«˜æ•ˆåœ°è®¡ç®—$P(x_{t+1} \mid x_{t-1})$ï¼š

$$
\begin{aligned}
P(x_{t+1} \mid x_{t-1})
&= \frac{\sum_{x_t} P(x_{t+1}, x_t, x_{t-1})}{P(x_{t-1})}\\
&= \frac{\sum_{x_t} P(x_{t+1} \mid x_t, x_{t-1}) P(x_t, x_{t-1})}{P(x_{t-1})}\\
&= \sum_{x_t} P(x_{t+1} \mid x_t) P(x_t \mid x_{t-1})
\end{aligned}
$$

åˆ©ç”¨è¿™ä¸€äº‹å®ï¼Œæˆ‘ä»¬åªéœ€è¦è€ƒè™‘è¿‡å»è§‚å¯Ÿä¸­çš„ä¸€ä¸ªéå¸¸çŸ­çš„å†å²ï¼š
$P(x_{t+1} \mid x_t, x_{t-1}) = P(x_{t+1} \mid x_t)$ã€‚

### å› æœå…³ç³»

 æ•°æ®å­˜åœ¨ä¸€ä¸ªè‡ªç„¶çš„æ–¹å‘ï¼Œå³åœ¨æ—¶é—´ä¸Šæ˜¯å‰è¿›çš„

## è®­ç»ƒ

```python
tau = 4
features = torch.zeros((T - tau, tau))
for i in range(tau):
    features[:, i] = x[i: T - tau + i]
labels = x[tau:].reshape((-1, 1))
```

## é¢„æµ‹

é¦–å…ˆæ˜¯æ£€æŸ¥[**æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥**]çš„èƒ½åŠ›ï¼Œ ä¹Ÿå°±æ˜¯_å•æ­¥é¢„æµ‹_ï¼ˆone-step-ahead predictionï¼‰ã€‚

```python
onestep_preds = net(features)
d2l.plot([time, time[tau:]],
         [x.detach().numpy(), onestep_preds.detach().numpy()], 'time',
         'x', legend=['data', '1-step preds'], xlim=[1, 1000],
         figsize=(6, 3))
```

é€šå¸¸ï¼Œå¯¹äºç›´åˆ°ğ‘¥ğ‘¡çš„è§‚æµ‹åºåˆ—ï¼Œå…¶åœ¨æ—¶é—´æ­¥ğ‘¡+ğ‘˜å¤„çš„é¢„æµ‹è¾“å‡ºğ‘¥Ì‚Â ğ‘¡+ğ‘˜Â ç§°ä¸ºğ‘˜_æ­¥é¢„æµ‹_ï¼ˆğ‘˜-step-ahead-predictionï¼‰ã€‚
å¤šæ­¥é¢„æµ‹

```python
multistep_preds = torch.zeros(T)
multistep_preds[: n_train + tau] = x[: n_train + tau]
for i in range(n_train + tau, T):
    multistep_preds[i] = net(
        multistep_preds[i - tau:i].reshape((1, -1)))
```
``
```python
d2l.plot([time, time[tau:], time[n_train + tau:]],
         [x.detach().numpy(), onestep_preds.detach().numpy(),
          multistep_preds[n_train + tau:].detach().numpy()], 'time',
         'x', legend=['data', '1-step preds', 'multistep preds'],
         xlim=[1, 1000], figsize=(6, 3))
```

kæ­¥é¢„æµ‹

```python
max_steps = 64

features = torch.zeros((T - tau - max_steps + 1, tau + max_steps))
# åˆ—iï¼ˆi<tauï¼‰æ˜¯æ¥è‡ªxçš„è§‚æµ‹ï¼Œå…¶æ—¶é—´æ­¥ä»ï¼ˆiï¼‰åˆ°ï¼ˆi+T-tau-max_steps+1ï¼‰
for i in range(tau):
    features[:, i] = x[i: i + T - tau - max_steps + 1]

# åˆ—iï¼ˆi>=tauï¼‰æ˜¯æ¥è‡ªï¼ˆi-tau+1ï¼‰æ­¥çš„é¢„æµ‹ï¼Œå…¶æ—¶é—´æ­¥ä»ï¼ˆiï¼‰åˆ°ï¼ˆi+T-tau-max_steps+1ï¼‰
for i in range(tau, tau + max_steps):
    features[:, i] = net(features[:, i - tau:i]).reshape(-1)
    
    
steps = (1, 4, 16, 64)
d2l.plot([time[tau + i - 1: T - max_steps + i] for i in steps],
         [features[:, tau + i - 1].detach().numpy() for i in steps], 'time', 'x',
         legend=[f'{i}-step preds' for i in steps], xlim=[5, 1000],
         figsize=(6, 3))

```

## å°ç»“

- å†…æ’æ³•ï¼ˆåœ¨ç°æœ‰è§‚æµ‹å€¼ä¹‹é—´è¿›è¡Œä¼°è®¡ï¼‰å’Œå¤–æ¨æ³•ï¼ˆå¯¹è¶…å‡ºå·²çŸ¥è§‚æµ‹èŒƒå›´è¿›è¡Œé¢„æµ‹ï¼‰åœ¨å®è·µçš„éš¾åº¦ä¸Šå·®åˆ«å¾ˆå¤§ã€‚å› æ­¤ï¼Œå¯¹äºæ‰€æ‹¥æœ‰çš„åºåˆ—æ•°æ®ï¼Œåœ¨è®­ç»ƒæ—¶å§‹ç»ˆè¦å°Šé‡å…¶æ—¶é—´é¡ºåºï¼Œå³æœ€å¥½ä¸è¦åŸºäºæœªæ¥çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚
- åºåˆ—æ¨¡å‹çš„ä¼°è®¡éœ€è¦ä¸“é—¨çš„ç»Ÿè®¡å·¥å…·ï¼Œä¸¤ç§è¾ƒæµè¡Œçš„é€‰æ‹©æ˜¯è‡ªå›å½’æ¨¡å‹å’Œéšå˜é‡è‡ªå›å½’æ¨¡å‹ã€‚
- å¯¹äºæ—¶é—´æ˜¯å‘å‰æ¨è¿›çš„å› æœæ¨¡å‹ï¼Œæ­£å‘ä¼°è®¡é€šå¸¸æ¯”åå‘ä¼°è®¡æ›´å®¹æ˜“ã€‚
- å¯¹äºç›´åˆ°æ—¶é—´æ­¥ğ‘¡çš„è§‚æµ‹åºåˆ—ï¼Œå…¶åœ¨æ—¶é—´æ­¥ğ‘¡+ğ‘˜çš„é¢„æµ‹è¾“å‡ºæ˜¯â€œğ‘˜æ­¥é¢„æµ‹â€ã€‚éšç€æˆ‘ä»¬å¯¹é¢„æµ‹æ—¶é—´ğ‘˜å€¼çš„å¢åŠ ï¼Œä¼šé€ æˆè¯¯å·®çš„å¿«é€Ÿç´¯ç§¯å’Œé¢„æµ‹è´¨é‡çš„æé€Ÿä¸‹é™ã€‚