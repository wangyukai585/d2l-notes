
在这一节中，我们将从零开始实现整个方法， 包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。

 同时，了解更细致的工作原理将方便我们自定义模型、自定义层或自定义损失函数。 在这一节中，我们将只使用张量和自动求导。

## 1 生成数据集

我们的任务是使用这个有限样本的数据集来恢复这个模型的参数。

```python
def synthetic_data(w, b, num_examples):  #@save
    """生成y = Xw + b + 噪声"""
    X = torch.normal(0, 1, (num_examples, len(w)))  # 生成1000个样本，每个样本有len(w)个特征（这里是2个），服从N(0,1)
    y = torch.matmul(X, w) + b                     # 按线性模型计算出真实y
    y += torch.normal(0, 0.01, y.shape)            # 给y加上少量噪声，使数据更真实
    return X, y.reshape((-1, 1))                   # 返回特征矩阵X和标签列向量y

true_w = torch.tensor([2, -3.4])  # 设定真实权重
true_b = 4.2                      # 设定真实偏置
features, labels = synthetic_data(true_w, true_b, 1000)  # 生成1000条数据
```

## 2 读取数据集

训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型

定义一个`data_iter`函数， 该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为`batch_size`的小批量。 每个小批量包含一组特征和标签。

```python
def data_iter(batch_size, features, labels):
    num_examples = len(features)                 # 样本总数，比如1000
    indices = list(range(num_examples))          # 生成 [0, 1, 2, ..., 999]
    random.shuffle(indices)                      # 打乱顺序，保证每次取数据随机
    for i in range(0, num_examples, batch_size): # 每次取 batch_size 个样本
        batch_indices = torch.tensor(
            indices[i: min(i + batch_size, num_examples)])  # 当前这批样本的索引
        yield features[batch_indices], labels[batch_indices]  # 返回一批数据（特征和标签）
```

在深度学习框架中实现的内置迭代器效率要高得多， 它可以处理存储在文件中的数据和数据流提供的数据。

## 3 初始化模型参数

```python
w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

在我们开始用小批量随机梯度下降优化我们的模型参数之前， 我们需要先有一些参数。

## 4 定义模型

```python
def linreg(X, w, b):  #@save
    """线性回归模型"""
    return torch.matmul(X, w) + b
```

## 5 定义损失函数

```python
def squared_loss(y_hat, y):  #@save
    """均方损失"""
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
```

## 6 定义优化算法

尽管线性回归有解析解，但本书中的其他模型却没有。

小批量随机梯度下降

```python
def sgd(params, lr, batch_size):  #@save
    """小批量随机梯度下降"""
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

## 7 训练

理解这段代码至关重要，因为从事深度学习后， 相同的训练过程几乎一遍又一遍地出现。

在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。 计算完损失后，我们开始反向传播，存储每个参数的梯度。 最后，我们调用优化算法`sgd`来更新模型参数。

![[Pasted image 20251026211726.png]]

```python
# 超参数设定
lr = 0.03           # 学习率（learning rate），控制每次参数更新的步幅
num_epochs = 3      # 训练轮数（epoch），即完整遍历数据的次数

# 模型与损失函数
net = linreg         # 线性回归模型：y_hat = Xw + b
loss = squared_loss  # 均方损失函数（Mean Squared Error）

# 训练循环
for epoch in range(num_epochs):  # 外层循环：共训练 num_epochs 轮
    for X, y in data_iter(batch_size, features, labels):  # 内层循环：逐批读取数据
        l = loss(net(X, w, b), y)       # 计算当前批次的损失 l = (y_hat - y)^2 / 2
        l.sum().backward()              # 对损失求和，再反向传播计算梯度
        sgd([w, b], lr, batch_size)     # 使用小批量随机梯度下降（SGD）更新参数 w 和 b

    # 每一轮训练完后，计算当前模型在全部数据上的平均损失
    with torch.no_grad():               # 关闭梯度追踪，加快计算速度
        train_l = loss(net(features, w, b), labels)   # 整个数据集上的损失
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')  # 输出当前轮的平均损失
```

```python
grad_fn=<SubBackward0>
```

表示这是由自动求导系统（autograd）计算出来的张量，它仍然保留了梯度计算图；

## 8 小结

- 我们学习了深度网络是如何实现和优化的。在这一过程中只使用张量和自动微分，不需要定义层或复杂的优化器。
    
- 这一节只触及到了表面知识。在下面的部分中，我们将基于刚刚介绍的概念描述其他模型，并学习如何更简洁地实现其他模型。

## 9 练习

为什么只要可导就可以用自动微分 自动微分的本质是什么 简单回答
### 一句话回答

因为自动微分（autograd）能按照计算图逐步应用链式法则，自动算出任意可导函数的梯度。

1. 程序里每一步（加、乘、sin、exp 等）都是**基本可导运算**；
2. 自动微分会在计算时**记录这些运算关系**，形成一张“计算图”；
3. 反向传播时，它从输出往回应用**链式法则（chain rule）**，一步步乘回来，就得到每个变量的导数。