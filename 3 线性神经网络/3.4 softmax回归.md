通常，机器学习实践者用_分类_这个词来描述两个有微妙差别的问题： 1. 我们只对样本的“硬性”类别感兴趣，即属于哪个类别； 2. 我们希望得到“软性”类别，即得到属于每个类别的概率。 这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。

## 3.4.1 分类问题

选择如何表示标签

类别之间的自然顺序

表示分类数据的简单方法：_独热编码_（one-hot encoding），独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0

## 3.4.2 网络架构

与线性回归一样，softmax回归也是一个单层神经网络。softmax回归的输出层也是全连接层。
$$
\begin{split}\begin{aligned}
o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\
o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\
o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.
\end{aligned}\end{split}
$$
$\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$

## 3.4.3 全连接层的参数开销


具体来说，对于任何具有d个输入和q个输出的全连接层， 参数开销为O(dq)
可减少到dq/n，平衡参数节约和模型有效性

## 3.4.4 softmax运算

要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。

 此外，我们需要一个训练的目标函数，来激励模型精准地估计概率。我们希望这些样本是刚好有一半实际上属于预测的类别。 这个属性叫做_校准_（calibration）。

softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质

$$
\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}
$$
尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个_线性模型_（linear model）。

## 3.4.5 小批量样本的矢量化

$$
\begin{split}\begin{aligned} \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ \hat{\mathbf{Y}} & = \mathrm{softmax}(\mathbf{O}). \end{aligned}\end{split}
$$

## 3.4.6 损失函数

接下来，我们需要一个损失函数来度量预测的效果。 我们将使用最大似然估计。

### 对数似然

softmax函数给出了一个向量yhat， 我们可以将其视为“对给定任意输入x的每个类的条件概率”

![[Pasted image 20251026224008.png]]

交叉熵cross-entropy loss
$$
l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j.
$$
解释：
![[Pasted image 20251026224352.png]]

### softmax及其导数

$$
\begin{split}\begin{aligned}
l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\
&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\
&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.
\end{aligned}\end{split}
$$


考虑相对于任何未规范化的预测oj的导数，我们得到：

$$
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
$$

换句话说，导数是我们softmax模型分配的概率与实际发生的情况（由独热标签向量表示）之间的差异。 从这个意义上讲，这与我们在回归中看到的非常相似， 其中梯度是观测值和估计值之间的差异。

在任何指数族分布模型中 ，对数似然的梯度正是由此得出的。

### 交叉熵损失

对于标签，我们可以使用与以前相同的表示形式。 唯一的区别是，我们现在用一个概率向量表示(0.1,0.2,0.7)

此损失称为_交叉熵损失_（cross-entropy loss）,它是分类问题最常用的损失之一。

## 3.4.7 信息论基础

_信息论_（information theory）涉及编码、解码、发送以及尽可能简洁地处理信息或数据。

### 熵

信息论的核心思想是量化数据中的信息内容。 在信息论中，该数值被称为分布P的_熵_（entropy）。

$H[P] = \sum_j - P(j) \log P(j).$

它表示：
**一个随机变量（分布 P）所包含的不确定性（信息量）**。

当对数底为 **e** 时，单位叫 **nat（纳特）**；

$1\ \text{nat} = \frac{1}{\log 2} \approx 1.44\ \text{bit}.$
分布越均匀、越难预测，熵就越大；越确定、越容易预测，熵就越小。

### 信息量

 克劳德·香农决定用信息量$\log \frac{1}{P(j)} = -\log P(j)$ 来量化这种惊异程度.
定义的熵， 是当分配的概率真正匹配数据生成过程时的_信息量的期望_。

### 重新审视交叉熵

![[Pasted image 20251026225836.png]]

我们可以从两方面来考虑交叉熵分类目标： （i）最大化观测数据的似然；（ii）最小化传达标签所需的惊异。

交叉熵
$$
H(y, \hat{y}) = -\sum_j y_j \log \hat{y}_j
$$
就是所有样本惊异的平均值。

## 3.4.8 模型预测和评估

在接下来的实验中，我们将使用_精度_（accuracy）来评估模型的性能。 精度等于正确预测数与预测总数之间的比率。

## 3.4.9 小结

- softmax运算获取一个向量并将其映射为概率。
    
- softmax回归适用于分类问题，它使用了softmax运算中输出类别的概率分布。
    
- 交叉熵是一个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数。

## 3.4.10 练习

太复杂不看了