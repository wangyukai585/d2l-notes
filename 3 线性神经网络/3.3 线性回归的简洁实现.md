本节将介绍如何通过使用深度学习框架来简洁地实现 [3.2节](https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#sec-linear-scratch)中的线性回归模型。

## 1 生成数据集

```python
import numpy as np
import torch
from torch.utils import data
from d2l import torch as d2l

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)
```

## 2 读取数据集

我们可以调用框架中现有的API来读取数据。 我们将`features`和`labels`作为API的参数传递，并通过数据迭代器指定`batch_size`。 此外，布尔值`is_train`表示是否希望数据迭代器对象在每个迭代周期内打乱数据。

```python
def load_array(data_arrays, batch_size, is_train=True):  #@save
    """构造一个PyTorch数据迭代器"""
    
    # 1️⃣ 用 TensorDataset 将特征和标签打包成一个数据集对象
    # data_arrays 是一个元组 (features, labels)
    # TensorDataset 会把它们配对成 (X[i], y[i]) 的形式
    dataset = data.TensorDataset(*data_arrays)
    
    # 2️⃣ DataLoader 用于按 batch（批次）从 dataset 中取数据
    # 参数说明：
    #   - dataset：刚才创建的数据集
    #   - batch_size：每次取多少个样本
    #   - shuffle：是否打乱样本顺序（训练集通常要打乱）
    return data.DataLoader(dataset, batch_size, shuffle=is_train)


# 设置批量大小为10
batch_size = 10

# 调用 load_array 构建数据迭代器
# 它会在训练时每次返回10条 (X, y) 样本
data_iter = load_array((features, labels), batch_size)
```

这里我们使用`iter`构造Python迭代器，并使用`next`从迭代器中获取第一项。

```python
next(iter(data_iter))
```

## 3 定义模型

对于标准深度学习模型，我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。

我们首先定义一个模型变量`net`，它是一个`Sequential`类的实例。 `Sequential`类将多个层串联在一起。 当给定输入数据时，`Sequential`实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。

单层网络架构， 这一单层被称为_全连接层_（fully-connected layer）， 因为它的每一个输入都通过矩阵-向量乘法得到它的每个输出。

```python
# nn是神经网络的缩写
from torch import nn

net = nn.Sequential(nn.Linear(2, 1))
```
在PyTorch中，全连接层在`Linear`类中定义。 值得注意的是，我们将两个参数传递到`nn.Linear`中。 第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1。

## 4 初始化模型参数

我们能直接访问参数以设定它们的初始值。 我们通过`net[0]`选择网络中的第一个图层， 然后使用`weight.data`和`bias.data`方法访问参数。 我们还可以使用替换方法`normal_`和`fill_`来重写参数值。

```python
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)
```

## 5 定义损失函数

计算均方误差使用的是`MSELoss`类，也称为平方L2范数。 默认情况下，它返回所有样本损失的平均值。
loss = nn.MSELoss()

## 6 定义优化算法

小批量随机梯度下降算法是一种优化神经网络的标准工具， PyTorch在`optim`模块中实现了该算法的许多变种。 当我们实例化一个`SGD`实例时，我们要指定优化的参数 （可通过`net.parameters()`从我们的模型中获得）以及优化算法所需的超参数字典。 小批量随机梯度下降只需要设置`lr`值，这里设置为0.03。

```python
trainer = torch.optim.SGD(net.parameters(), lr=0.03)
```

## 7 训练

通过深度学习框架的高级API来实现我们的模型只需要相对较少的代码。

在每个迭代周期里，我们将完整遍历一次数据集（`train_data`）， 不停地从中获取一个小批量的输入和相应的标签。 对于每一个小批量，我们会进行以下步骤:

- 通过调用`net(X)`生成预测并计算损失`l`（前向传播）。
    
- 通过进行反向传播来计算梯度。
    
- 通过调用优化器来更新模型参数。

```python
num_epochs = 3  # 训练轮数（epoch），即完整遍历整个数据集的次数

for epoch in range(num_epochs):  # 外层循环：进行多轮训练
    for X, y in data_iter:       # 内层循环：每次从 data_iter 中取出一个小批量 (X, y)
        l = loss(net(X), y)      # 计算当前 batch 的损失值 l = loss(预测值, 真实值)
        
        trainer.zero_grad()      # 清空上一次的梯度，否则梯度会累加
        l.backward()             # 反向传播：计算当前损失对所有参数的梯度
        trainer.step()           # 用优化器（trainer）更新参数，比如执行一次 SGD 或 Adam
        
    # 每一轮结束后，用全部数据计算整体损失，评估当前模型表现
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')  # 打印当前轮的平均损失
```
> **循环训练模型 → 计算损失 → 反向传播 → 更新参数 → 打印每轮的损失结果。**

## 8 小结

- 我们可以使用PyTorch的高级API更简洁地实现模型。
    
- 在PyTorch中，`data`模块提供了数据处理工具，`nn`模块定义了大量的神经网络层和常见损失函数。
    
- 我们可以通过`_`结尾的方法将参数替换，从而初始化参数。

## 9 练习


