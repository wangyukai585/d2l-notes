
我们发现通过深度学习框架的高级API能够使实现线性回归变得更加容易。 同样，通过深度学习框架的高级API也能更方便地实现softmax回归模型。

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```
## 3.7.1 初始化模型参数

```python
# PyTorch不会隐式地调整输入的形状。因此，
# 我们在线性层前定义了展平层（flatten），来调整网络输入的形状
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);
```

## 3.7.2 重新审视softmax的实现

我们计算了模型的输出，然后将此输出送入交叉熵损失
指数可能会造成数值稳定性问题

$\exp(o_k)$可能大于数据类型容许的最大数字，即_上溢_（overflow）
inf无穷大
nan不是数字
$$
\hat y_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}
$$
$$
\begin{split}\begin{aligned}
\hat y_j & =  \frac{\exp(o_j - \max(o_k))\exp(\max(o_k))}{\sum_k \exp(o_k - \max(o_k))\exp(\max(o_k))} \\
& = \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}.
\end{aligned}\end{split}
$$

但是
$$
\exp(o_j - \max(o_k))
$$
_下溢_（underflow）。 这些值可能会四舍五入为零
-inf nan

计算交叉熵损失时会取它们的对数
将softmax和交叉熵结合在一起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题

$$
\begin{split}\begin{aligned}
\log{(\hat y_j)} & = \log\left( \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}\right) \\
& = \log{(\exp(o_j - \max(o_k)))}-\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)} \\
& = o_j - \max(o_k) -\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)}.
\end{aligned}\end{split}
$$

```python
loss = nn.CrossEntropyLoss(reduction='none')
```
## 3.7.3 优化算法

```python
trainer = torch.optim.SGD(net.parameters(), lr=0.1)
```

## 3.7.4 训练

```python
num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

## 3.7.5 小结

- 使用深度学习框架的高级API，我们可以更简洁地实现softmax回归。
    
- 从计算的角度来看，实现softmax回归比较复杂。在许多情况下，深度学习框架在这些著名的技巧之外采取了额外的预防措施，来确保数值的稳定性。这使我们避免了在实践中从零开始编写模型时可能遇到的陷阱。

## 3.7.6 练习

测试精度在迭代周期增加后下降，是因为**模型过拟合（overfitting）**。

训练时间太长时，模型逐渐“记住”训练数据中的噪声与细节，失去了对新数据（测试集）的泛化能力。

**解决方法：**
1. **早停法（Early stopping）**：在验证集精度不再提升时停止训练。
2. **正则化（Regularization）**：如 L2 权重衰减、Dropout 等。
3. **增加数据量或数据增强（Data augmentation）**。
4. **减小模型复杂度（Simpler model）**。