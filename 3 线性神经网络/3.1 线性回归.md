前言：
在介绍深度神经网络之前，我们需要了解神经网络训练的基础知识。 本章我们将介绍神经网络的整个训练过程， 包括：定义简单的神经网络架构、数据处理、指定损失函数和如何训练模型。 为了更容易学习，我们将从经典算法————_线性_神经网络开始，介绍神经网络的基础知识。 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络， 这些知识将为本书其他部分中更复杂的技术奠定基础。


_回归_（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。

分类问题的目标是预测数据属于一组类别中的哪一个。

## 1 线性回归的基本元素

 线性回归基于几个简单的假设： 首先，假设自变量x和因变量y之间的关系是线性的， 即y可以表示为x中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。

_训练数据集_（training data set） 或_训练集_（training set）
_样本_（sample）， 也可以称为_数据点_（data point）或_数据样本_（data instance）

 我们把试图预测的目标（比如预测房屋价格）称为_标签_（label）或_目标_（target）。 预测所依据的自变量（面积和房龄）称为_特征_（feature）或_协变量_（covariate）。
![[Pasted image 20251026165514.png]]

### 线性模型

线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和

w_权重_（weight） b_偏置_（bias）、_偏移量_（offset）或_截距_（intercept）

输入特征的一个 _仿射变换_（affine transformation）。 仿射变换的特点是通过加权和对特征进行_线性变换_（linear transformation）， 并通过偏置项来进行_平移_（translation）。

输出的预测值由输入特征通过_线性模型_的仿射变换决定，仿射变换由所选权重和偏置确定。

在开始寻找最好的_模型参数_（model parameters）和之前， 我们还需要两个东西： （1）一种模型质量的度量方式； （2）一种能够更新模型以提高模型预测质量的方法。

### 损失函数

_损失函数_（loss function）能够量化目标的_实际_值与_预测_值之间的差距。

![[Pasted image 20251026181226.png]]

![[Pasted image 20251026181233.png]]
### 解析解

线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解（analytical solution）

- X 是特征矩阵（每一行是一个样本，每一列是一个特征）
L(w) = ||y - Xw||²

**推导过程：**
1. 对损失函数 L(w) 对 w 求导：
    ∂L/∂w = -2Xᵀ(y - Xw)
2. 令导数为 0（表示损失最小）：
    Xᵀy = XᵀXw
3. 解得：
    w* = (XᵀX)⁻¹ Xᵀy

### 随机梯度下降

本书中我们用到一种名为_梯度下降_（gradient descent）的方法， 这种方法几乎可以优化所有深度学习模型。 它通过不断地在损失函数递减的方向上更新参数来降低误差。

梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。 但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做_小批量随机梯度下降_（minibatch stochastic gradient descent）。

![[Pasted image 20251026182922.png]]

总结一下，算法的步骤如下： （1）初始化模型参数的值，如随机初始化； （2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。 对于平方损失和仿射变换，我们可以明确地写成如下形式:

![[Pasted image 20251026183102.png]]

w x向量 向量表示法 β每个小批量的样本数 批量大小 batch size yita表示学习率learning rate

这些可以调整但不在训练过程中更新的参数称为_超参数_（hyperparameter）。 _调参_（hyperparameter tuning）是选择超参数的过程。 超参数通常是我们根据训练迭代结果来调整的， 而训练迭代结果是在独立的_验证数据集_（validation dataset）上评估得到的。

估计值也不会使损失函数真正地达到最小值。 因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值。

像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。
事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为_泛化_（generalization）。

### 用模型进行预测

给定特征估计目标的过程通常称为_预测_（prediction）或_推断_（inference）。


## 2 矢量化加速

训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。 为了实现这一点，需要我们对计算进行矢量化， 从而利用线性代数库，而不是在Python中编写开销高昂的for循环。

timer计时器

 矢量化代码通常会带来数量级的加速
 
## 3 正态分布与平方损失

正态分布（normal distribution），也称为_高斯分布_（Gaussian distribution），

![[Pasted image 20251026185354.png]]

```python
def normal(x, mu, sigma):
    p = 1 / math.sqrt(2 * math.pi * sigma**2)
    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)
```

均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是： 我们假设了观测中包含噪声，其中噪声服从正态分布。 噪声正态分布如下式:
$y = \mathbf{w}^\top \mathbf{x} + b + \epsilon,$
$\epsilon \sim \mathcal{N}(0, \sigma^2)$

$$
P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).
$$
$$
P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)}|\mathbf{x}^{(i)}).
$$
根据极大似然估计法选择的估计量称为_极大似然估计量_。
最大化似然对数
\_最小化负对数_

$$
-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.
$$

现在我们只需要假设sigma是某个固定常数就可以忽略第一项

在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。

> 最大似然估计（Maximum Likelihood Estimation, MLE）的含义可以一句话概括：
> 	**在已知模型形式的情况下，选择一组参数，使得“在这些参数下观测到的样本数据出现的概率最大”。**

## 4 从线性回归到深度网络

### 神经网络图

将线性回归模型描述为一个神经网络

![[Pasted image 20251026202741.png]]该图只显示连接模式，即只显示每个输入如何连接到输出，隐去了权重和偏置的值

线性回归是一个单层神经网络。

输入层中的_输入数_（或称为_特征维度_，feature dimensionality）

由于模型重点在发生计算的地方，所以通常我们在计算层数时不考虑输入层。

神经网络的_层数_为1。 我们可以将线性回归模型视为仅由单个人工神经元组成的神经网络，或称为单层神经网络。

对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换中的输出层） 称为_全连接层_（fully-connected layer）或称为_稠密层_（dense layer）。

### 生物学

![[Pasted image 20251026203255.png]]


当今大多数深度学习的研究几乎没有直接从神经科学中获得灵感。 我们援引斯图尔特·罗素和彼得·诺维格在他们的经典人工智能教科书 _Artificial Intelligence:A Modern Approach_ ([Russell and Norvig, 2016](https://zh.d2l.ai/chapter_references/zreferences.html#id141 "Russell, S. J., & Norvig, P. (2016). Artificial intelligence: a modern approach. Malaysia; Pearson Education Limited,.")) 中所说的：虽然飞机可能受到鸟类的启发，但几个世纪以来，鸟类学并不是航空创新的主要驱动力。 同样地，如今在深度学习中的灵感同样或更多地来自数学、统计学和计算机科学。

## 5 小结

- 机器学习模型中的关键要素是训练数据、损失函数、优化算法，还有模型本身。
    
- 矢量化使数学表达上更简洁，同时运行的更快。
    
- 最小化目标函数和执行极大似然估计等价。
    
- 线性回归模型也是一个简单的神经网络。

## 6 练习

不写了 把AI生成的公式复制到笔记里很麻烦 自己看看好了

