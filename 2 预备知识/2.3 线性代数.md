
## 1 标量

严格来说，仅包含一个数值被称为_标量_（scalar）。
符号𝑐和𝑓称为_变量_（variable），它们表示未知的标量值。

本书采用了数学表示法，其中标量变量由普通小写字母表示（例如，𝑥、𝑦和𝑧）。 本书用ℝ表示所有（连续）_实数_标量的空间，

标量由只有一个元素的张量表示。

## 2 向量

向量可以被视为标量值组成的列表。 这些标量值被称为向量的_元素_（element）或_分量_（component）。

在数学表示法中，向量通常记为粗体、小写的符号 （例如，𝐱、𝐲和𝐳)）。

大量文献认为列向量是向量的默认方向，在本书中也是如此。

![[Pasted image 20251019131247.png]]

## 3 长度、维度和形状

向量的长度通常称为向量的_维度_（dimension）。

len()
x.shape

_向量_或_轴_的维度被用来表示_向量_或_轴_的长度，即向量或轴的元素数量。 然而，张量的维度用来表示张量具有的轴数。 在这个意义上，张量的某个轴的维数就是这个轴的长度。

## 4 矩阵

A.T 转置

## 5 张量

张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的𝑛维数组的通用方法。

 例如，向量是一阶张量，矩阵是二阶张量。 张量用特殊字体的大写字母表示（例如，𝖷、𝖸和𝖹）， 它们的索引机制（例如𝑥𝑖𝑗𝑘和[𝖷]1,2𝑖−1,3）与矩阵类似。

## 6 张量算法的基本性质

**两个矩阵的按元素乘法称为_Hadamard积_（Hadamard product）（数学符号⊙）**。

## 7 降维

默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。

我们还可以**指定张量沿哪一个轴来通过求和降低维度**。

A_sum_axis0 = A.sum(axis=0)

指定`axis=1`将通过汇总所有列的元素降维（轴1）。

沿0/1轴降维

A.sum(axis=[0, 1])  # 结果和A.sum()相同

mean()
sum() numel()

axis 表示“被压缩/被操作的轴”，而不是保留的轴。

### 非降维求和

计算总和或均值时保持轴数不变
sum_A = A.sum(axis=1, keepdims=True)

## 8 点积

np.dot(x, y)
等价
np.sum(x * y)

加权平均

## 9 矩阵-向量积

在代码中使用张量表示矩阵-向量积，我们使用`mv`函数。
当我们为矩阵`A`和向量`x`调用`torch.mv(A, x)`

## 10 矩阵-矩阵乘法

torch.mm(A, B)

矩阵-矩阵乘法可以简单地称为**矩阵乘法**，不应与“Hadamard积”混淆。

## 11 范数

向量的_范数_是表示一个向量有多大。 这里考虑的_大小_（size）概念不涉及维度，而是分量的大小。

向量范数要满足一些属性。

- 第一个性质是：如果我们按常数因子缩放向量的所有元素， 其范数也会按相同常数因子的_绝对值_缩放
- 熟悉的三角不等式
- 范数必须是非负的

欧几里得距离是一个L2范数，向量元素平方和的平方根
u = torch.tensor([3.0, -4.0])
torch.norm(u)

L1范数，向量元素的绝对值之和
torch.abs(u).sum()

Lp范数
_Frobenius范数_
![[Pasted image 20251026000125.png]]

torch.norm(torch.ones((4, 9)))

### 范数和目标

在深度学习中，我们经常试图解决优化问题： _最大化_分配给观测数据的概率; _最小化_预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。

## 12 关于线性代数的更多信息

矩阵可以分解为因子，这些分解可以显示真实世界数据集中的低维结构。

机器学习的整个子领域都侧重于使用矩阵分解及其向高阶张量的泛化，来发现数据集中的结构并解决预测问题。

## 13 小结

- 标量、向量、矩阵和张量是线性代数中的基本数学对象。
    
- 向量泛化自标量，矩阵泛化自向量。
    
- 标量、向量、矩阵和张量分别具有零、一、二和任意数量的轴。
    
- 一个张量可以通过`sum`和`mean`沿指定的轴降低维度。
    
- 两个矩阵的按元素乘法被称为他们的Hadamard积。它与矩阵乘法不同。
    
- 在深度学习中，我们经常使用范数，如范数、范数和Frobenius范数。
    
- 我们可以对标量、向量、矩阵和张量执行各种操作。

## 14 练习

![[Pasted image 20251026000844.png]]

**3.** 是的。对任意方阵 A，

$(A+A^\top)^\top=A^\top+(A^\top)^\top=A^\top+A=A+A^\top$

因此 A+A^\top 恒为**对称矩阵**。


**4.** 若 X 的形状是 (2,3,4)，len(X) 的结果是 **2**（即第 0 维的长度）。

  

**5.** 对任意张量 X，len(X) 总是返回 **轴 0（第一维）** 的长度。

  

**6.** A / A.sum(axis=1) 通常会报 **广播错误**，因为 A.sum(axis=1) 形状为 (m,)，与 A 的 (m,n) 在尾维不兼容。

正确的行归一化写法：A / A.sum(axis=1, keepdims=True) 或 A / A.sum(axis=1).reshape(-1,1)。

  

**7.** 张量形状 (2,3,4)：

- 沿轴 0 求和 → 形状 **(3,4)**
    
- 沿轴 1 求和 → 形状 **(2,4)**
    
- 沿轴 2 求和 → 形状 **(2,3)**
    

  

**8.** 对于 np.linalg.norm，若**不指定 ord/axis**，对任意形状张量都会先**展平**，再计算**欧几里得范数（2-范数）**：

$\|X\|=\sqrt{\sum_{i} x_i^2}.$


（若想在某些轴上求范数，需用 axis= 指定；ndim>2 时一般只支持 ord=None 或 ord='fro' 配合两个轴。）