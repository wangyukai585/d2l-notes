è‡ªæ³¨æ„åŠ›åŒæ—¶å…·æœ‰å¹¶è¡Œè®¡ç®—å’Œæœ€çŸ­çš„æœ€å¤§è·¯å¾„é•¿åº¦è¿™ä¸¤ä¸ªä¼˜åŠ¿ã€‚

å°½ç®¡Transformeræœ€åˆæ˜¯åº”ç”¨äºåœ¨æ–‡æœ¬æ•°æ®ä¸Šçš„åºåˆ—åˆ°åºåˆ—å­¦ä¹ ï¼Œä½†ç°åœ¨å·²ç»æ¨å¹¿åˆ°å„ç§ç°ä»£çš„æ·±åº¦å­¦ä¹ ä¸­ï¼Œä¾‹å¦‚è¯­è¨€ã€è§†è§‰ã€è¯­éŸ³å’Œå¼ºåŒ–å­¦ä¹ é¢†åŸŸã€‚

## æ¨¡å‹

![[Pasted image 20251118134737.png]]

ä»å®è§‚è§’åº¦æ¥çœ‹ï¼ŒTransformerçš„ç¼–ç å™¨æ˜¯ç”±å¤šä¸ªç›¸åŒçš„å±‚å åŠ è€Œæˆçš„ï¼Œæ¯ä¸ªå±‚éƒ½æœ‰ä¸¤ä¸ªå­å±‚ï¼ˆå­å±‚è¡¨ç¤ºä¸ºsublayerï¼‰ã€‚ç¬¬ä¸€ä¸ªå­å±‚æ˜¯_å¤šå¤´è‡ªæ³¨æ„åŠ›_ï¼ˆmulti-head self-attentionï¼‰æ±‡èšï¼›ç¬¬äºŒä¸ªå­å±‚æ˜¯_åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ_ï¼ˆpositionwise feed-forward networkï¼‰ã€‚

å— :numref:`sec_resnet`ä¸­æ®‹å·®ç½‘ç»œçš„å¯å‘ï¼Œæ¯ä¸ªå­å±‚éƒ½é‡‡ç”¨äº†_æ®‹å·®è¿æ¥_ï¼ˆresidual connectionï¼‰ã€‚

åœ¨æ®‹å·®è¿æ¥çš„åŠ æ³•è®¡ç®—ä¹‹åï¼Œç´§æ¥ç€åº”ç”¨_å±‚è§„èŒƒåŒ–_ï¼ˆlayer normalizationï¼‰

å› æ­¤ï¼Œè¾“å…¥åºåˆ—å¯¹åº”çš„æ¯ä¸ªä½ç½®ï¼ŒTransformerç¼–ç å™¨éƒ½å°†è¾“å‡ºä¸€ä¸ªğ‘‘ç»´è¡¨ç¤ºå‘é‡ã€‚

Transformerè§£ç å™¨ä¹Ÿæ˜¯ç”±å¤šä¸ªç›¸åŒçš„å±‚å åŠ è€Œæˆçš„ï¼Œå¹¶ä¸”å±‚ä¸­ä½¿ç”¨äº†æ®‹å·®è¿æ¥å’Œå±‚è§„èŒƒåŒ–ã€‚é™¤äº†ç¼–ç å™¨ä¸­æè¿°çš„ä¸¤ä¸ªå­å±‚ä¹‹å¤–ï¼Œè§£ç å™¨è¿˜åœ¨è¿™ä¸¤ä¸ªå­å±‚ä¹‹é—´æ’å…¥äº†ç¬¬ä¸‰ä¸ªå­å±‚ï¼Œç§°ä¸º_ç¼–ç å™¨ï¼è§£ç å™¨æ³¨æ„åŠ›_ï¼ˆencoder-decoder attentionï¼‰å±‚ã€‚

åœ¨ç¼–ç å™¨ï¼è§£ç å™¨æ³¨æ„åŠ›ä¸­ï¼ŒæŸ¥è¯¢æ¥è‡ªå‰ä¸€ä¸ªè§£ç å™¨å±‚çš„è¾“å‡ºï¼Œè€Œé”®å’Œå€¼æ¥è‡ªæ•´ä¸ªç¼–ç å™¨çš„è¾“å‡ºã€‚åœ¨è§£ç å™¨è‡ªæ³¨æ„åŠ›ä¸­ï¼ŒæŸ¥è¯¢ã€é”®å’Œå€¼éƒ½æ¥è‡ªä¸Šä¸€ä¸ªè§£ç å™¨å±‚çš„è¾“å‡ºã€‚ä½†æ˜¯ï¼Œè§£ç å™¨ä¸­çš„æ¯ä¸ªä½ç½®åªèƒ½è€ƒè™‘è¯¥ä½ç½®ä¹‹å‰çš„æ‰€æœ‰ä½ç½®ã€‚è¿™ç§_æ©è”½_ï¼ˆmaskedï¼‰æ³¨æ„åŠ›ä¿ç•™äº†_è‡ªå›å½’_ï¼ˆauto-regressiveï¼‰å±æ€§ï¼Œç¡®ä¿é¢„æµ‹ä»…ä¾èµ–äºå·²ç”Ÿæˆçš„è¾“å‡ºè¯å…ƒã€‚

## åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ

åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œå¯¹åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®çš„è¡¨ç¤ºè¿›è¡Œå˜æ¢æ—¶ä½¿ç”¨çš„æ˜¯åŒä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼Œè¿™å°±æ˜¯ç§°å‰é¦ˆç½‘ç»œæ˜¯_åŸºäºä½ç½®çš„_ï¼ˆpositionwiseï¼‰çš„åŸå› ã€‚

```python
#@save
class PositionWiseFFN(nn.Module):
    """åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ"""
    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,
                 **kwargs):
        super(PositionWiseFFN, self).__init__(**kwargs)
        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)
        self.relu = nn.ReLU()
        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)

    def forward(self, X):
        return self.dense2(self.relu(self.dense1(X)))
```

## æ®‹å·®è¿æ¥å’Œå±‚è§„èŒƒåŒ–

ç”±æ®‹å·®è¿æ¥å’Œç´§éšå…¶åçš„å±‚è§„èŒƒåŒ–ç»„æˆçš„ã€‚ä¸¤è€…éƒ½æ˜¯æ„å»ºæœ‰æ•ˆçš„æ·±åº¦æ¶æ„çš„å…³é”®ã€‚

**AddNorm = å¯¹ Y åš dropoutï¼Œç„¶ååŠ ä¸ŠåŸè¾“å…¥ Xï¼ˆæ®‹å·®è¿æ¥ï¼‰ï¼Œæœ€ååš LayerNorm ç¨³å®šè¾“å‡ºã€‚**

```python
#@save
class AddNorm(nn.Module):
    """æ®‹å·®è¿æ¥åè¿›è¡Œå±‚è§„èŒƒåŒ–"""
    def __init__(self, normalized_shape, dropout, **kwargs):
        super(AddNorm, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)
        self.ln = nn.LayerNorm(normalized_shape)

    def forward(self, X, Y):
        return self.ln(self.dropout(Y) + X)
```

## ç¼–ç å™¨

`EncoderBlock`ç±»åŒ…å«ä¸¤ä¸ªå­å±‚ï¼šå¤šå¤´è‡ªæ³¨æ„åŠ›å’ŒåŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œï¼Œè¿™ä¸¤ä¸ªå­å±‚éƒ½ä½¿ç”¨äº†æ®‹å·®è¿æ¥å’Œç´§éšçš„å±‚è§„èŒƒåŒ–ã€‚

```python
#@save
class EncoderBlock(nn.Module):
    """Transformerç¼–ç å™¨å—"""
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
                 dropout, use_bias=False, **kwargs):
        super(EncoderBlock, self).__init__(**kwargs)
        self.attention = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout,
            use_bias)
        self.addnorm1 = AddNorm(norm_shape, dropout)
        self.ffn = PositionWiseFFN(
            ffn_num_input, ffn_num_hiddens, num_hiddens)
        self.addnorm2 = AddNorm(norm_shape, dropout)

    def forward(self, X, valid_lens):
        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))
        return self.addnorm2(Y, self.ffn(Y))
```

FFN = **Position-wise Feed-Forward Network**

```python
#@save
class EncoderBlock(nn.Module):
    """Transformerç¼–ç å™¨å—"""
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
                 dropout, use_bias=False, **kwargs):
        super(EncoderBlock, self).__init__(**kwargs)
        self.attention = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout,
            use_bias)
        self.addnorm1 = AddNorm(norm_shape, dropout)
        self.ffn = PositionWiseFFN(
            ffn_num_input, ffn_num_hiddens, num_hiddens)
        self.addnorm2 = AddNorm(norm_shape, dropout)

    def forward(self, X, valid_lens):
        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))
        return self.addnorm2(Y, self.ffn(Y))
```

**Transformerç¼–ç å™¨ä¸­çš„ä»»ä½•å±‚éƒ½ä¸ä¼šæ”¹å˜å…¶è¾“å…¥çš„å½¢çŠ¶**

ç”±äºè¿™é‡Œä½¿ç”¨çš„æ˜¯å€¼èŒƒå›´åœ¨âˆ’1å’Œ1ä¹‹é—´çš„å›ºå®šä½ç½®ç¼–ç ï¼Œå› æ­¤é€šè¿‡å­¦ä¹ å¾—åˆ°çš„è¾“å…¥çš„åµŒå…¥è¡¨ç¤ºçš„å€¼éœ€è¦å…ˆä¹˜ä»¥åµŒå…¥ç»´åº¦çš„å¹³æ–¹æ ¹è¿›è¡Œé‡æ–°ç¼©æ”¾ï¼Œç„¶åå†ä¸ä½ç½®ç¼–ç ç›¸åŠ ã€‚

## è§£ç å™¨

**Transformerè§£ç å™¨ä¹Ÿæ˜¯ç”±å¤šä¸ªç›¸åŒçš„å±‚ç»„æˆ**

åœ¨`DecoderBlock`ç±»ä¸­å®ç°çš„æ¯ä¸ªå±‚åŒ…å«äº†ä¸‰ä¸ªå­å±‚ï¼šè§£ç å™¨è‡ªæ³¨æ„åŠ›ã€â€œç¼–ç å™¨-è§£ç å™¨â€æ³¨æ„åŠ›å’ŒåŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œã€‚è¿™äº›å­å±‚ä¹Ÿéƒ½è¢«æ®‹å·®è¿æ¥å’Œç´§éšçš„å±‚è§„èŒƒåŒ–å›´ç»•ã€‚

**num_hiddens = æ¯ä¸ª token çš„éšè—å‘é‡ï¼ˆç‰¹å¾å‘é‡ï¼‰çš„ç»´åº¦ã€‚**
hidden size
embedding dimension

## è®­ç»ƒ

```python
num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10
lr, num_epochs, device = 0.005, 200, d2l.try_gpu()
ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4
key_size, query_size, value_size = 32, 32, 32
norm_shape = [32]

train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)

encoder = TransformerEncoder(
    len(src_vocab), key_size, query_size, value_size, num_hiddens,
    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
    num_layers, dropout)
decoder = TransformerDecoder(
    len(tgt_vocab), key_size, query_size, value_size, num_hiddens,
    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
    num_layers, dropout)
net = d2l.EncoderDecoder(encoder, decoder)
d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)
```

ç¼–ç å™¨è‡ªæ³¨æ„åŠ›æƒé‡çš„å½¢çŠ¶ä¸ºï¼ˆç¼–ç å™¨å±‚æ•°ï¼Œæ³¨æ„åŠ›å¤´æ•°ï¼Œ`num_steps`æˆ–æŸ¥è¯¢çš„æ•°ç›®ï¼Œ`num_steps`æˆ–â€œé”®ï¼å€¼â€å¯¹çš„æ•°ç›®ï¼‰ã€‚


## å°ç»“

- Transformeræ˜¯ç¼–ç å™¨ï¼è§£ç å™¨æ¶æ„çš„ä¸€ä¸ªå®è·µï¼Œå°½ç®¡åœ¨å®é™…æƒ…å†µä¸­ç¼–ç å™¨æˆ–è§£ç å™¨å¯ä»¥å•ç‹¬ä½¿ç”¨ã€‚
- åœ¨Transformerä¸­ï¼Œå¤šå¤´è‡ªæ³¨æ„åŠ›ç”¨äºè¡¨ç¤ºè¾“å…¥åºåˆ—å’Œè¾“å‡ºåºåˆ—ï¼Œä¸è¿‡è§£ç å™¨å¿…é¡»é€šè¿‡æ©è”½æœºåˆ¶æ¥ä¿ç•™è‡ªå›å½’å±æ€§ã€‚
- Transformerä¸­çš„æ®‹å·®è¿æ¥å’Œå±‚è§„èŒƒåŒ–æ˜¯è®­ç»ƒéå¸¸æ·±åº¦æ¨¡å‹çš„é‡è¦å·¥å…·ã€‚
- Transformeræ¨¡å‹ä¸­åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œä½¿ç”¨åŒä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼Œä½œç”¨æ˜¯å¯¹æ‰€æœ‰åºåˆ—ä½ç½®çš„è¡¨ç¤ºè¿›è¡Œè½¬æ¢ã€‚



