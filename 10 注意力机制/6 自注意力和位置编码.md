每个查询都会关注所有的键－值对并生成一个注意力输出。 由于查询、键和值来自同一组输入，因此被称为 _自注意力_（self-attention），也被称为_内部注意力_（intra-attention）

![[PixPin_2025-11-17_23-58-00.png]]
![[PixPin_2025-11-18_00-06-19.png]]

## 自注意力

给定一个由词元组成的输入序列$\mathbf{x}_1, \ldots, \mathbf{x}_n$，
其中任意$\mathbf{x}_i \in \mathbb{R}^d$（$1 \leq i \leq n$）。
该序列的自注意力输出为一个长度相同的序列
$\mathbf{y}_1, \ldots, \mathbf{y}_n$，其中：

$$\mathbf{y}_i = f(\mathbf{x}_i, (\mathbf{x}_1, \mathbf{x}_1), \ldots, (\mathbf{x}_n, \mathbf{x}_n)) \in \mathbb{R}^d$$

根据 :eqref:`eq_attn-pooling`中定义的注意力汇聚函数$f$。
下面的代码片段是基于多头注意力对一个张量完成自注意力的计算，
张量的形状为（批量大小，时间步的数目或词元序列的长度，$d$）。
输出与输入的张量形状相同。

## 比较卷积神经网络、循环神经网络和自注意力

![[Pasted image 20251118132157.png]]


总而言之，卷积神经网络和自注意力都拥有并行计算的优势， 而且自注意力的最大路径长度最短。 但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。

## 位置编码

假设输入表示$\mathbf{X} \in \mathbb{R}^{n \times d}$
包含一个序列中$n$个词元的$d$维嵌入表示。
位置编码使用相同形状的位置嵌入矩阵
$\mathbf{P} \in \mathbb{R}^{n \times d}$输出$\mathbf{X} + \mathbf{P}$，
矩阵第$i$行、第$2j$列和$2j+1$列上的元素为：

$$\begin{aligned} p_{i, 2j} &= \sin\left(\frac{i}{10000^{2j/d}}\right),\\p_{i, 2j+1} &= \cos\left(\frac{i}{10000^{2j/d}}\right).\end{aligned}$$

```python
#@save
class PositionalEncoding(nn.Module):
    """位置编码"""
    def __init__(self, num_hiddens, dropout, max_len=1000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(dropout)
        # 创建一个足够长的P
        self.P = torch.zeros((1, max_len, num_hiddens))
        X = torch.arange(max_len, dtype=torch.float32).reshape(
            -1, 1) / torch.pow(10000, torch.arange(
            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)
        self.P[:, :, 0::2] = torch.sin(X)
        self.P[:, :, 1::2] = torch.cos(X)

    def forward(self, X):
        X = X + self.P[:, :X.shape[1], :].to(X.device)
        return self.dropout(X)
```

在位置嵌入矩阵𝐏中， [**行代表词元在序列中的位置，列代表位置编码的不同维度**]

## 绝对位置信息

为了明白沿着编码维度单调降低的频率与绝对位置信息的关系

在二进制表示中，较高比特位的交替频率低于较低比特位。

在二进制表示中，较高比特位的交替频率低于较低比特位， 与下面的热图所示相似，只是位置编码通过使用三角函数[**在编码维度上降低频率**]。 由于输出是浮点数，因此此类连续表示比二进制表示法更节省空间。

![[PixPin_2025-11-18_13-35-35.png]]

**和二进制的类比（完全对应）**
- 二进制最低 bit：01010101（变化最快）
- 更高 bit：00001111（变化慢）
- 再高 bit：0000000011111111（更慢）

位置编码就是用连续 sin/cos 把这个规律重现。
**左边对应低 bit（变化快）**
**右边对应高 bit（变化慢）**
但因为是浮点连续值 → 表达能力更强。

## 相对位置信息

**位置编码用 sin/cos 的好处是：只要知道位置差 δ，就能用一个固定的 2×2 旋转矩阵，把“位置 i 的编码”线性映射到“位置 i+δ 的编码”。**

令$\omega_j = 1/10000^{2j/d}$，
对于任何确定的位置偏移$\delta$，

![[PixPin_2025-11-18_13-41-10.png]]

## 小结

- 在自注意力中，查询、键和值都来自同一组输入。
- 卷积神经网络和自注意力都拥有并行计算的优势，而且自注意力的最大路径长度最短。但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。
- 为了使用序列的顺序信息，可以通过在输入表示中添加位置编码，来注入绝对的或相对的位置信息。


