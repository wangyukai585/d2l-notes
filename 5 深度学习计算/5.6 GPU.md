```python
!nvidia-smi
```

在PyTorch中，每个数组都有一个设备（device）， 我们通常将其称为环境（context）。

## 计算设备

我们可以指定用于存储和计算的设备，如CPU和GPU。 默认情况下，张量是在内存中创建的，然后使用CPU计算它。

```python
import torch
from torch import nn

torch.device('cpu'), torch.device('cuda'), torch.device('cuda:1')
```

我们可以查询可用gpu的数量。
```python
torch.cuda.device_count()
```

```python
def try_gpu(i=0):  #@save
    """如果存在，则返回gpu(i)，否则返回cpu()"""
    if torch.cuda.device_count() >= i + 1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')

def try_all_gpus():  #@save
    """返回所有可用的GPU，如果没有GPU，则返回[cpu(),]"""
    devices = [torch.device(f'cuda:{i}')
             for i in range(torch.cuda.device_count())]
    return devices if devices else [torch.device('cpu')]

try_gpu(), try_gpu(10), try_all_gpus()
```

## 张量与GPU

需要注意的是，无论何时我们要对多个项进行操作， 它们都必须在同一个设备上。 例如，如果我们对两个张量求和， 我们需要确保两个张量都位于同一个设备上， 否则框架将不知道在哪里存储结果，甚至不知道在哪里执行计算。

### 存储在GPU上

X = torch.ones(2, 3, device=try_gpu())

Y = torch.rand(2, 3, device=try_gpu(1))

### 复制

![[Pasted image 20251101221414.png]]

```python
Z = X.cuda(1)
print(X)
print(Z)
```

假设变量`Z`已经存在于第二个GPU上。 如果我们还是调用`Z.cuda(1)`会发生什么？ 它将返回`Z`，而不会复制并分配新内存。

```python
Z.cuda(1) is Z
```
True

### 旁注

人们使用GPU来进行机器学习，因为单个GPU相对运行速度快。 但是在设备（CPU、GPU和其他机器）之间传输数据比计算慢得多。 这也使得并行化变得更加困难。

最后，当我们打印张量或将张量转换为NumPy格式时， 如果数据不在内存中，框架会首先将其复制到内存中， 这会导致额外的传输开销。 更糟糕的是，它现在受制于全局解释器锁，使得一切都得等待Python完成。

## 神经网络与GPU

```python
net = nn.Sequential(nn.Linear(3, 1))
net = net.to(device=try_gpu())
```
总之，只要所有的数据和参数都在同一个设备上， 我们就可以有效地学习模型。
net(X)
net[0].weight.data.device

## 小结

- 我们可以指定用于存储和计算的设备，例如CPU或GPU。默认情况下，数据在主内存中创建，然后使用CPU进行计算。    
- 深度学习框架要求计算的所有输入数据都在同一设备上，无论是CPU还是GPU。
- 不经意地移动数据可能会显著降低性能。一个典型的错误如下：计算GPU上每个小批量的损失，并在命令行中将其报告给用户（或将其记录在NumPy `ndarray`中）时，将触发全局解释器锁，从而使所有GPU阻塞。最好是为GPU内部的日志分配内存，并且只移动较大的日志。